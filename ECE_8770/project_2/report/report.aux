\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@refcontext{nty/global//global/global}
\abx@aux@cite{0}{zhang2023dive}
\abx@aux@segm{0}{0}{zhang2023dive}
\abx@aux@cite{0}{zhang2023dive}
\abx@aux@segm{0}{0}{zhang2023dive}
\@writefile{toc}{\contentsline {section}{\numberline {1}Technical Description}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Recurrent Neural Networks}{1}{subsection.1.1}\protected@file@percent }
\newlabel{eqn: h_t rnn}{{1}{1}{Recurrent Neural Networks}{equation.1.1}{}}
\abx@aux@cite{0}{zhang2023dive}
\abx@aux@segm{0}{0}{zhang2023dive}
\abx@aux@cite{0}{zhang2023dive}
\abx@aux@segm{0}{0}{zhang2023dive}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \blx@tocontentsinit {0}\cite {zhang2023dive}: On the left recurrent connections are depicted via cyclic edges. On the right, we unfold the RNN over time steps. Here, recurrent edges span adjacent time steps, while conventional connections are computed synchronously}}{2}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig: rnns v1}{{1}{2}{\cite {zhang2023dive}: On the left recurrent connections are depicted via cyclic edges. On the right, we unfold the RNN over time steps. Here, recurrent edges span adjacent time steps, while conventional connections are computed synchronously}{figure.caption.1}{}}
\newlabel{eqn: o_t rnn}{{2}{2}{Recurrent Neural Networks}{equation.1.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}Backpropagation through time (Backpropagation for RNNs)}{2}{subsubsection.1.1.1}\protected@file@percent }
\newlabel{eqn: total loss rnn}{{3}{2}{Backpropagation through time (Backpropagation for RNNs)}{equation.1.3}{}}
\abx@aux@cite{0}{zhang2023dive}
\abx@aux@segm{0}{0}{zhang2023dive}
\abx@aux@cite{0}{tallec2017unbiasing}
\abx@aux@segm{0}{0}{tallec2017unbiasing}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \blx@tocontentsinit {0}\cite {zhang2023dive}: Computational graph showing dependencies for an RNN model with three time steps. Boxes represent variables (not shaded) or parameters (shaded) and circles represent operators.}}{3}{figure.caption.2}\protected@file@percent }
\newlabel{fig: rnns v2}{{2}{3}{\cite {zhang2023dive}: Computational graph showing dependencies for an RNN model with three time steps. Boxes represent variables (not shaded) or parameters (shaded) and circles represent operators}{figure.caption.2}{}}
\newlabel{eqn: rnn lt wrt w_h}{{4}{3}{Backpropagation through time (Backpropagation for RNNs)}{equation.1.4}{}}
\newlabel{eqn: rnn h_t wrt w_h}{{5}{3}{Backpropagation through time (Backpropagation for RNNs)}{equation.1.5}{}}
\newlabel{eqn: rnn total loss wrt w_h}{{8}{3}{Backpropagation through time (Backpropagation for RNNs)}{equation.1.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Comparing strategies for computing gradients in RNNs. From top to bottom: randomized truncation, regular truncation, and full computation.}}{4}{figure.caption.3}\protected@file@percent }
\newlabel{fig: truncated_bptt}{{3}{4}{Comparing strategies for computing gradients in RNNs. From top to bottom: randomized truncation, regular truncation, and full computation}{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Long Short Term Memory nets (LSTMs)}{4}{subsection.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces lstms}}{4}{figure.caption.4}\protected@file@percent }
\newlabel{fig: lstms}{{4}{4}{lstms}{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}RNN/LSTM configurations}{4}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Experiments and Results}{4}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Training details}{4}{subsection.2.1}\protected@file@percent }
\abx@aux@read@bbl@mdfivesum{B37FF35E95E6BBE5126F2A880DC969DA}
\abx@aux@defaultrefcontext{0}{tallec2017unbiasing}{nty/global//global/global}
\abx@aux@defaultrefcontext{0}{zhang2023dive}{nty/global//global/global}
\gdef \@abspage@last{6}
