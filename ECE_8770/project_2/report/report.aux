\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@refcontext{nty/global//global/global}
\abx@aux@cite{0}{zhang2023dive}
\abx@aux@segm{0}{0}{zhang2023dive}
\abx@aux@cite{0}{zhang2023dive}
\abx@aux@segm{0}{0}{zhang2023dive}
\@writefile{toc}{\contentsline {section}{\numberline {1}Technical Description}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Recurrent Neural Networks}{1}{subsection.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Fig 1: \blx@tocontentsinit {0}\cite {zhang2023dive}: On the left recurrent connections are depicted via cyclic edges. On the right, we unfold the RNN over time steps. Here, recurrent edges span adjacent time steps, while conventional connections are computed synchronously}}{1}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig: rnns v1}{{1}{1}{Fig 1: \cite {zhang2023dive}: On the left recurrent connections are depicted via cyclic edges. On the right, we unfold the RNN over time steps. Here, recurrent edges span adjacent time steps, while conventional connections are computed synchronously}{figure.caption.1}{}}
\abx@aux@cite{0}{zhang2023dive}
\abx@aux@segm{0}{0}{zhang2023dive}
\abx@aux@cite{0}{zhang2023dive}
\abx@aux@segm{0}{0}{zhang2023dive}
\newlabel{eqn: h_t rnn}{{1}{2}{Recurrent Neural Networks}{equation.1.1}{}}
\newlabel{eqn: o_t rnn}{{2}{2}{Recurrent Neural Networks}{equation.1.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}Backpropagation through time (Backpropagation for RNNs)}{2}{subsubsection.1.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Fig 2: \blx@tocontentsinit {0}\cite {zhang2023dive}: Computational graph showing dependencies for an RNN model with three time steps. Boxes represent variables (not shaded) or parameters (shaded) and circles represent operators.}}{2}{figure.caption.2}\protected@file@percent }
\newlabel{fig: rnns v2}{{2}{2}{Fig 2: \cite {zhang2023dive}: Computational graph showing dependencies for an RNN model with three time steps. Boxes represent variables (not shaded) or parameters (shaded) and circles represent operators}{figure.caption.2}{}}
\newlabel{eqn: total loss rnn}{{3}{3}{Backpropagation through time (Backpropagation for RNNs)}{equation.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Long Short Term Memory nets (LSTMs)}{3}{subsection.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces lstms}}{3}{figure.caption.3}\protected@file@percent }
\newlabel{fig: lstms}{{3}{3}{lstms}{figure.caption.3}{}}
\abx@aux@read@bbl@mdfivesum{9D18A4EBB6A60D13CEE7F8C45DB5E348}
\abx@aux@defaultrefcontext{0}{zhang2023dive}{nty/global//global/global}
\gdef \@abspage@last{4}
