\section{Technical Description}
% Main goals
% Operation of General Neural Networks
% MLP
% CNN
% Activation functions
% Optimizers
% Loss functions

The project described henceforth is one that aims to understand the working `principle' of a neural network. In principle,
a neural network has several core components; the inputs and outputs, the computational units known as `neurons', 
the weights, the activation function, an optimizer and an error function. A neural network's components work together 
to form composite functions that ultimately combine to generate a final, function. Thus objective of `understanding' 
the working principle of a neural network implies the attempt to understand how the core components behave in different
configurations. With this aim in mind, this project contains two parts; Part 1- `Sanity Check': In this part, 
4 multi-layered perceptrons of different configurations were employed to classify a synthetic, linearly separable 
classes of 3 classes. Part 2- `MLP vs CNN': In this part, the prediction performance of a convolutional neural net (CNN)
and an MLP (Multi-layered perceptrons) were compared. 

In the following sections will start with an explanation on the operation of a neural network. Then, activation
functions and their derivatives shall be mathematically described and illustrated. Then, the algorithms used 
in this project for optimizing the neural networks shall be described, which are
\textbf{Stochastic Gradient Descent (SGD)} and \textbf{Adaptive Momentum (Adam)}. Finally the so called `learning task' 
with respect to the datasets in both parts will be defined.

\subsection{Operation of Multi-layered Perceptrons}

A more in-depth treatment of the operation is done in ...

\subsection{Operation of Convolutional Neural Networks}

\subsection{Activation functions}

\subsection{Optimizers}

\subsubsection{Stochastic Gradient Descent}

\subsubsection{Adaptive Momentum}
